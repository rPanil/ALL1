{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DIGIT MNIST**"
      ],
      "metadata": {
        "id": "aYPBteP7XHjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo4P96V7GpgE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "###\n",
        "digit_mnist = keras.datasets.mnist\n",
        "(x_train_full,y_train_full),(x_test,y_test) = digit_mnist.load_data()\n",
        "\n",
        "###\n",
        "\n",
        "x_train_n = x_train_full/255\n",
        "x_test_n = x_test/255\n",
        "\n",
        "####\n",
        "x_valid, x_train = x_train_n[:6000], x_train_n[6000:]\n",
        "y_valid, y_train = y_train_full[:6000] , y_train_full[6000:]\n",
        "x_test = x_test_n\n",
        "\n",
        "####\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
        "model.add(keras.layers.Dense(200, activation = 'relu'))\n",
        "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
        "model.add(keras.layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "####\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "###\n",
        "model.fit(x_train,y_train, epochs = 60, validation_data = (x_valid,y_valid))\n",
        "\n",
        "####\n",
        "\n",
        "model.evaluate(x_test,y_test)\n",
        "\n",
        "####x_new = x_test[:4]\n",
        "predict_new = model.predict(x_new)\n",
        "predict_new.round(2)\n",
        "\n",
        "####\n",
        "predict_n = model.predict(x_new)\n",
        "classes_x = np.argmax(predict_n,axis=1)\n",
        "classes_x\n",
        "\n",
        "###\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread('path',0)\n",
        "img= cv2.resize(img,(28,28))\n",
        "img = img /255.0\n",
        "img= cv2.reshape([1,28,28,1])\n",
        "\n",
        "predict = model.predict(img)\n",
        "predict_value = predict.argmax()\n",
        "print('the value within image is {predict_value}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Facerecog CNN**"
      ],
      "metadata": {
        "id": "-R0KIFHQt9Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "###\n",
        "TrainingImagePath='/content/drive/MyDrive/Colab Notebooks/Face Images/Final Training Images'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(shear_range=0.1,zoom_range=0.1,horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator()\n",
        "training_set = train_datagen.flow_from_directory(TrainingImagePath,target_size=(64, 64),batch_size=32,class_mode='categorical')\n",
        "test_set = test_datagen.flow_from_directory(TrainingImagePath,target_size=(64, 64),batch_size=32,class_mode='categorical')\n",
        "print(test_set.class_indices)\n",
        "\n",
        "###\n",
        "TrainClasses=training_set.class_indices\n",
        "\n",
        "ResultMap={}\n",
        "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
        "    ResultMap[faceValue]=faceName\n",
        "\n",
        "import pickle\n",
        "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
        "    pickle.dump(ResultMap, fileWriteStream)\n",
        "\n",
        "print(\"Mapping Of Face Images With It's ID : \",ResultMap)\n",
        "\n",
        "###\n",
        "classifier= Sequential()\n",
        "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(64, activation='relu'))\n",
        "classifier.add(Dense(16, activation='softmax'))\n",
        "classifier.compile(loss='categorical_crossentropy',optimizer = 'adam',metrics=[\"accuracy\"])\n",
        "\n",
        "###\n",
        "classifier.fit(training_set,epochs=20,validation_data=test_set)\n",
        "\n",
        "###\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ImagePath='/content/drive/MyDrive/Colab Notebooks/Face Images/Final Testing Images/face4/3face4.jpg'\n",
        "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
        "plt.imshow(test_image)\n",
        "test_image=image.img_to_array(test_image)\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "result=classifier.predict(test_image,verbose=0)\n",
        "print('Prediction is: ',ResultMap[np.argmax(result)])"
      ],
      "metadata": {
        "id": "7I3vfIAcuA75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fashion MNIST**"
      ],
      "metadata": {
        "id": "ruAvxDqpqR0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "\n",
        "###\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train_full.shape\n",
        "plt.imshow(x_train_full[1])\n",
        "plt.show()\n",
        "\n",
        "###\n",
        "x_train_n = x_train_full / 255.\n",
        "x_test_n = x_test / 255.\n",
        "x_train = x_train_n\n",
        "y_train = y_train_full\n",
        "x_valid = x_train_n[:6000]\n",
        "y_valid = y_train_full[:6000]\n",
        "x_test = x_test_n\n",
        "\n",
        "###\n",
        "model = keras.models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_s\n",
        "hape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "###\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "metrics=['accuracy'],\n",
        "optimizer = opt)\n",
        "model_r= model.fit(x_train,y_train,epochs=20,validation_data=(x_valid,y_valid))\n",
        "###\n",
        "scores = model.evaluate(x_test,y_test)\n",
        "print(\"Accuracy ==>\",scores[1]*100,\"%\")\n",
        "print(\"Loss ==>\",scores[0]*100,\"%\")\n",
        "\n",
        "###\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import colorama\n",
        "from colorama import Fore, Back, Style\n",
        "Labels = [\"TShirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"B\n",
        "ag\",\"Ankle Boot\"]\n",
        "image = Image.open(\"TShirt1.jpeg\")\n",
        "print()\n",
        "print(Fore.RED+Style.BRIGHT+\"ORIGINAL INPUT IMAGE --->\")\n",
        "Loss ==> 41.15738272666931 %\n",
        "[9 2 1 1 6]\n",
        "1/1 [==============================] - 0s 153ms/step\n",
        "Out[11]:\n",
        "array([9, 2, 1, 1, 6])\n",
        "print()\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "print()\n",
        "image_array = np.asarray(image)\n",
        "resized = cv2.resize(image_array,(28,28))\n",
        "gray_scale = cv2.cvtColor(resized,cv2.COLOR_BGR2GRAY)\n",
        "temp = cv2.bitwise_not(gray_scale)\n",
        "print(Fore.RED+Style.BRIGHT+\"GRAY SCALED PIXELED IMAGE --->\")\n",
        "print()\n",
        "plt.imshow(temp)\n",
        "plt.show()\n",
        "print()\n",
        "print()\n",
        "x = np.argmax(model.predict(temp.reshape(1,28,28,1)))\n",
        "print(Fore.RED+Style.BRIGHT+\"Predicted Label For Test Image Is :\",Labels[x])"
      ],
      "metadata": {
        "id": "JGkLH1oqqV5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time Series RNN LSTM**"
      ],
      "metadata": {
        "id": "5Qra7cM_vCDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense,BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "\n",
        "###\n",
        "np.random.seed(42)\n",
        "num_data_points = 4000\n",
        "humidity = np.random.uniform(40, 80, num_data_points)\n",
        "temperature = np.random.uniform(20, 35, num_data_points)\n",
        "rainfall = np.random.uniform(0, 50, num_data_points)\n",
        "soil_moisture = np.random.uniform(20, 80, num_data_points)\n",
        "yield_ = 100 + 2 * humidity + 3 * temperature - 1.5 * rainfall + 0.5 * soil_moisture + np.random.normal(0, 10, num_data_points)\n",
        "df = pd.DataFrame({'humidity': humidity, 'temperature': temperature, 'rainfall': rainfall, 'soil_moisture': soil_moisture, 'yield': yield_})\n",
        "\n",
        "###\n",
        "X = df[['humidity', 'temperature', 'rainfall', 'soil_moisture']]\n",
        "y = df['yield'].values.reshape(-1, 1)\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "###\n",
        "sequence_length = 25\n",
        "X_sequences, y_sequences = [], []\n",
        "for i in range(len(X_scaled) - sequence_length):\n",
        "    X_sequences.append(X_scaled[i:i+sequence_length])\n",
        "    y_sequences.append(y_scaled[i+sequence_length])\n",
        "\n",
        "X_sequences, y_sequences = np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n",
        "\n",
        "###\n",
        "model = Sequential()\n",
        "model.add(LSTM(150, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "###\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "###\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, lr_schedule])\n",
        "\n",
        "###\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "\n",
        "###\n",
        "predictions_scaled = model.predict(X_test)\n",
        "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
        "\n",
        "###\n",
        "df_results = pd.DataFrame({'Actual': scaler_y.inverse_transform(y_test).flatten(), 'Predicted': predictions.flatten()})\n",
        "print(df_results.head(20))\n",
        "\n",
        "###\n",
        "mae = np.mean(np.abs(scaler_y.inverse_transform(y_test) - predictions))\n",
        "mse = np.mean((scaler_y.inverse_transform(y_test) - predictions) ** 2)"
      ],
      "metadata": {
        "id": "zmXrBOLlvCi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAN**"
      ],
      "metadata": {
        "id": "yWkSKQsPt-Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###\n",
        "np.random.seed(1000)\n",
        "\n",
        "noise_dim = 100\n",
        "batch_size = 64\n",
        "epochs = 10000\n",
        "\n",
        "###\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, input_dim=noise_dim, activation='relu'))\n",
        "generator.add(Dense(784, activation='sigmoid'))\n",
        "generator.add(Reshape((28, 28, 1)))\n",
        "\n",
        "###\n",
        "img_shape = (28, 28, 1)\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Flatten(input_shape=img_shape))\n",
        "discriminator.add(Dense(256, activation='relu'))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "###\n",
        "discriminator.trainable = False  # Fix the discriminator weights during GAN training\n",
        "gan_model = Sequential()\n",
        "gan_model.add(generator)\n",
        "gan_model.add(discriminator)\n",
        "gan_model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
        "\n",
        "###\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "\n",
        "\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "\n",
        "    idx = np.random.randint(0, 60000, batch_size)\n",
        "    real_images = x_train[idx]\n",
        "\n",
        "    labels_real = np.ones((batch_size, 1))\n",
        "    labels_fake = np.zeros((batch_size, 1))\n",
        "    labels_real += 0.05 * np.random.normal(size=labels_real.shape)  # Label smoothing for stability\n",
        "\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(real_images, labels_real)\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_images, labels_fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "    noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "    labels_gan = np.ones((batch_size, 1))\n",
        "    g_loss = gan_model.train_on_batch(noise, labels_gan)\n",
        "\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "\n",
        "        fig, axs = plt.subplots(5, 5)\n",
        "        cnt = 0\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                axs[i, j].imshow(generated_images[cnt, :, :, 0], cmap='gray')\n",
        "                axs[i, j].axis('off')\n",
        "                cnt += 1\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "BcCKzBU0t9dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Sentiment Analysis**"
      ],
      "metadata": {
        "id": "025WzrAwUA7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "max_features = 5000\n",
        "max_len = 300\n",
        "embedding_dim = 50\n",
        "lstm_units = 100\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    word_index = imdb.get_word_index()\n",
        "    words = [word_index.get(word, 0) for word in text.lower().split()]\n",
        "    input_sequence = sequence.pad_sequences([words], maxlen=max_len)\n",
        "\n",
        "    prediction = model.predict(input_sequence)[0, 0]\n",
        "\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    print(f\"Sentiment: {sentiment}, Confidence: {prediction}\")\n",
        "\n",
        "custom_text = \"I really enjoyed watching this movie, it was fantastic!\"\n",
        "predict_sentiment(custom_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "skADCIY7VDt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fbb5ec-3ad6-4873-b84c-5141cd3ccb96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 277s 697ms/step - loss: 0.5223 - accuracy: 0.7292 - val_loss: 0.3312 - val_accuracy: 0.8626\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 262s 670ms/step - loss: 0.2929 - accuracy: 0.8814 - val_loss: 0.3183 - val_accuracy: 0.8678\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 261s 669ms/step - loss: 0.2353 - accuracy: 0.9077 - val_loss: 0.3457 - val_accuracy: 0.8649\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.2050 - accuracy: 0.9223 - val_loss: 0.3998 - val_accuracy: 0.8561\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.1874 - accuracy: 0.9274 - val_loss: 0.3434 - val_accuracy: 0.8716\n",
            "782/782 [==============================] - 54s 68ms/step - loss: 0.3434 - accuracy: 0.8716\n",
            "Test Loss: 0.3434438705444336, Test Accuracy: 0.8715599775314331\n",
            "1/1 [==============================] - 1s 605ms/step\n",
            "Sentiment: Positive, Confidence: 0.5427003502845764\n"
          ]
        }
      ]
    }
  ]
}